{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm all IC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "import unidecode  # Este método quita las Ñ \n",
    "import unicodedata \n",
    "import math\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtaining test database codified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting APIs\n",
    "# Hacerlo con Año firma del contrato (menos) o año subida del SECOP (mas)?\n",
    "import requests\n",
    "url_secop = 'https://www.datos.gov.co/resource/c6dm-udt9.json'\n",
    "p_invias = {'nombre_de_la_entidad': 'INSTITUTO NACIONAL DE VÍAS (INVIAS)', \n",
    "            '$limit': 10000, \n",
    "            'causal_de_otras_formas_de_contratacion_directa': 'Contratos Interadministrativos (Literal C)'}\n",
    "r_invias = requests.get(url_secop, params = p_invias)\n",
    "\n",
    "# To .json\n",
    "d_invias = r_invias.json()\n",
    "# To df\n",
    "import pandas as pd\n",
    "df_invias_all = pd.DataFrame(d_invias)\n",
    "\n",
    "# Filter per year\n",
    "df_invias_all[['anno_firma_del_contrato']] = df_invias_all[['anno_firma_del_contrato']].apply(pd.to_numeric)\n",
    "df_invias = df_invias_all.loc[df_invias_all['anno_firma_del_contrato'] >= 2012]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pequeña caracterizacion de DB de IC en INVIAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacer grafica de los años de los IC y de las filas que tienen este field en null\n",
    "df_invias['anno_firma_del_contrato'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtener información de los municipios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting all municipios\n",
    "i_razsoc = df_invias['nom_raz_social_contratista'].tolist()\n",
    "\n",
    "i_mun = [ item for item in i_razsoc if 'MUNICIPIO' in item] + [ item for item in i_razsoc if 'DEPARTAMENTO' in item]\n",
    "i_mun = [ item for item in i_mun if 'ADMINISTRATIVO' not in item]\n",
    "i_mun = [ item for item in i_mun if 'AGENCIA' not in item]\n",
    "\n",
    "dff_invias = df_invias[df_invias['nom_raz_social_contratista'].isin(i_mun)]\n",
    "i_mun = dff_invias['nom_raz_social_contratista'].tolist()\n",
    "\n",
    "# API con division politico-administrativa\n",
    "url_deptos= 'https://www.datos.gov.co/resource/p95u-vi7k.json'\n",
    "p_deptos = {'$limit': 2000}\n",
    "r_deptos = requests.get(url_deptos, params = p_deptos)\n",
    "r_deptos.status_code\n",
    "r_deptos.url\n",
    "# To .json\n",
    "d_deptos = r_deptos.json()\n",
    "# To df\n",
    "df_deptos = pd.DataFrame(d_deptos)\n",
    "# To upper\n",
    "df_deptos['departamento'] = df_deptos['departamento'].str.upper()\n",
    "df_deptos['municipio'] = df_deptos['municipio'].str.upper()\n",
    "# Corregir casos particulares\n",
    "df_deptos['municipio'] = [re.sub('EL CARMEN DE BOLÍVAR','CARMEN DE BOLÍVAR',item) for item in df_deptos['municipio']] \n",
    "df_deptos['municipio'] = [re.sub('EL CARMEN DE VIBORAL','CARMEN DE VIBORAL',item) for item in df_deptos['municipio']] \n",
    "df_deptos['municipio'] = [re.sub('PROVIDENCIA','PROVIDENCIA Y SANTA CATALINA',item) for item in df_deptos['municipio']] \n",
    "df_deptos['municipio'] = [re.sub('ESPINAL','EL ESPINAL',item) for item in df_deptos['municipio']] \n",
    "df_deptos['municipio'] = [re.sub('ITAGUI','ITAGÜÍ',item) for item in df_deptos['municipio']] \n",
    "df_deptos['municipio'] = [re.sub('TOLÚ VIEJO','TOLUVIEJO',item) for item in df_deptos['municipio']] \n",
    "df_deptos['municipio'] = [re.sub('TIMBIQUÍ','TIMBIQUI',item) for item in df_deptos['municipio']] \n",
    "df_deptos['municipio'] = [re.sub('CHIPATÁ','CHIPATA',item) for item in df_deptos['municipio']] \n",
    "df_deptos['municipio'] = [re.sub('SUSACON','SUSACÓN',item) for item in df_deptos['municipio']] \n",
    "df_deptos['municipio'] = [re.sub('TIMBÍO','TIMBIO',item) for item in df_deptos['municipio']] \n",
    "df_deptos['municipio'] = [re.sub('CURITÍ','CURITI',item) for item in df_deptos['municipio']] \n",
    "df_deptos['departamento'] = [re.sub('ARCHIPIÉLAGO DE SAN ANDRÉS, PROVIDENCIA Y SANTA CATALINA','SAN ANDRÉS PROVIDENCIA Y SANTA CATALINA',item) for item in df_deptos['departamento']] \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STANDARIZE ALL LIST OF MUNICIPIOS\n",
    "\n",
    "# Taken from: https://stackoverflow.com/questions/14153364/reorder-string-using-regular-expressions\n",
    "def standarize_mun(mun):\n",
    "    if \"MUNICIPIO DE\" not in mun:  \n",
    "        mun = re.sub('MUNICIPIO','MUNICIPIO DE',mun)\n",
    "    nmun = re.sub('DEPARTAMENTO DE|DEPARTAEMNTO DE| EN EL DEPARTAMENTO DE | EN EL DEPARTAMENTO DEL',' - ',mun)     #Caso: DEPARTAMENTO DE\n",
    "    nmun = re.sub('(C/MARCA)',' - CUNDINAMARCA ', nmun)     #Caso: (C/MARCA)\n",
    "    # Remove double spaces and punctuation (except -) https://stackoverflow.com/questions/1546226/simple-way-to-remove-multiple-spaces-in-a-string\n",
    "    nmun = re.sub('\\.','',nmun)     # https://stackoverflow.com/questions/265960/best-way-to-strip-punctuation-from-a-string-in-python\n",
    "    nmun = re.sub(' +', ' ', nmun)\n",
    "    nmun = re.sub('[(){}<>]', '', nmun)   # Parentesis\n",
    "    r = re.compile('(^.*)(-)(.*$)')\n",
    "    nmun = r.sub(r'\\3'+ ' - ALCALDÍA '+ r'\\1',nmun)\n",
    "    nmun = nmun.lstrip()            # Remove spaces before beginning\n",
    "    nmun = re.sub('\\.','',nmun)\n",
    "    return nmun \n",
    "\n",
    "def standarize_depto(mun):\n",
    "    nmun = re.sub('GOBERNACION','',mun)     #Caso: DEPARTAMENTO DE\n",
    "    nmun = re.sub('DEPARTAMENTO DEL','DEPARTAMENTO DE',nmun)     #Caso: DEPARTAMENTO DE\n",
    "    nmun = re.sub('DEPARTAMENTO DE','GOBERNACIÓN -',nmun)     #Caso: DEPARTAMENTO DE\n",
    "    # Remove double spaces and punctuation (except -) https://stackoverflow.com/questions/1546226/simple-way-to-remove-multiple-spaces-in-a-string\n",
    "    nmun = re.sub('\\.','',nmun)     # https://stackoverflow.com/questions/265960/best-way-to-strip-punctuation-from-a-string-in-python\n",
    "    nmun = re.sub(' +', ' ', nmun)\n",
    "    nmun = re.sub('[(){}<>]', '', nmun)   # Parentesis\n",
    "    r = re.compile('(^.*)(-)(.*$)')\n",
    "    nmun = r.sub(r'\\3'+ ' - '+ r'\\1',nmun)\n",
    "    nmun = nmun.lstrip()            # Remove spaces before beginning\n",
    "    nmun = re.sub('\\.','',nmun)\n",
    "    nmun = re.sub(' +', ' ', nmun)\n",
    "    return nmun \n",
    "\n",
    "def strip_accents(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "              if not unicodedata.name(c).endswith('ACCENT')) \n",
    "\n",
    "i_mun = [strip_accents(item) for item in i_mun] #SIN TILDES Y DEJA LA Ñ!!\n",
    "# i_mun = [unidecode.unidecode(item) for item in i_mun] #SIN TILDES \n",
    "# i_mun = [unidecode.unidecode(item) for item in i_mun] #Añadir Ñ\n",
    "# List with standarized municipios: TODO ver casos especiales\n",
    "# i_mun = list(set(i_mun))\n",
    "i_muns = []\n",
    "\n",
    "# Corregir casos particulares\n",
    "i_mun = [re.sub('MUNICIPIO DEL CARMEN DE BOLIVAR','MUNICIPIO DE CARMEN DE BOLIVAR',item) for item in i_mun] \n",
    "i_mun = [re.sub('MUNICIPIO DE EL CARMEN DE BOLIVAR','MUNICIPIO DE CARMEN DE BOLIVAR',item) for item in i_mun] \n",
    "i_mun = [re.sub('MUNICIPIO DEL CARMEN DE BOLIVAR','MUNICIPIO DE CARMEN DE BOLIVAR',item) for item in i_mun] \n",
    "i_mun = [re.sub('MUNICIPIO DE EL CARMEN DE BOLIVAR','MUNICIPIO DE CARMEN DE BOLIVAR',item) for item in i_mun] \n",
    "i_mun = [re.sub('SANTIAGO DE CALI','CALI',item) for item in i_mun] \n",
    "i_mun = [re.sub('SAN JOSE DE CUCUTA','CUCUTA',item) for item in i_mun] \n",
    "i_mun = [re.sub('MUNICIPIO EL CERRITO','MUNICIPIO DE EL CERRITO',item) for item in i_mun] \n",
    "i_mun = [re.sub('LE RETEN','EL RETEN',item) for item in i_mun] \n",
    "i_mun = [re.sub('PROVIDENCIA Y SANTA CATALINA ISLAS','PROVIDENCIA Y SANTA CATALINA',item) for item in i_mun] \n",
    "i_mun = [re.sub('SAN JUAN BAUTISTA DE GUACARI','GUACARI',item) for item in i_mun] \n",
    "i_mun = [re.sub('SUSACON','SUSACÓN',item) for item in i_mun] \n",
    "\n",
    "\n",
    "for item in i_mun:\n",
    "    if 'MUNICIPIO' in item:\n",
    "        i_muns.append(standarize_mun(item))\n",
    "    else:\n",
    "        i_muns.append(standarize_depto(item))\n",
    "\n",
    "# Corregir errores particulares\n",
    "\n",
    "\n",
    "       \n",
    "\n",
    "#unaccented_string = unidecode.unidecode(accented_string)\n",
    "deptos_t = list(set(df_deptos['departamento'].tolist()))  # https://stackoverflow.com/questions/7961363/removing-duplicates-in-lists\n",
    "deptos_st = [strip_accents(item) for item in deptos_t]\n",
    "\n",
    "mun_t = list(set(df_deptos['municipio'].tolist())) # CON TILDES\n",
    "mun_st = [strip_accents(item) for item in mun_t] #SIN TILDES\n",
    "\n",
    "# Estandarizar tildes deptos\n",
    "for contd, depto in enumerate(deptos_st):\n",
    "    for i, item in enumerate(i_muns):\n",
    "        if depto in item:\n",
    "            i_muns[i] = re.sub(depto,deptos_t[contd],item)\n",
    "            \n",
    "# Estandarizar tildes mun\n",
    "for contm, mun in enumerate(mun_st):\n",
    "    for i, item in enumerate(i_muns):\n",
    "        if mun in item:\n",
    "            i_muns[i] = re.sub(mun,mun_t[contm],item)\n",
    "            \n",
    "            \n",
    "# Corregir casos particulares\n",
    "i_muns = [re.sub('MANÍZALES','MANIZALES',item) for item in i_muns]  \n",
    "i_muns = [re.sub('CUERQUIA','CUERQUÍA',item) for item in i_muns]  \n",
    "i_muns = [re.sub('ZAPAYAN','ZAPAYÁN',item) for item in i_muns]     \n",
    "\n",
    "# 1. Municipios que tienen el nombre del depto\n",
    "for i, item in enumerate(i_muns):\n",
    "    for contd, depto in enumerate(deptos_t):\n",
    "        if '-' in item: break     # Si ya esta estendarizado\n",
    "        item = item.lstrip()\n",
    "        if 'CARMEN DE BOLÍVAR' in item: continue\n",
    "        if depto in item:\n",
    "            # https://stackoverflow.com/questions/30232344/insert-a-string-before-a-substring-of-a-string\n",
    "            my_str = item\n",
    "            substr = depto\n",
    "            inserttxt = \" - \"\n",
    "            idx = my_str.index(substr)\n",
    "            i_muns[i] = my_str[:idx] + inserttxt + my_str[idx:]\n",
    "            i_muns[i] = standarize_mun(i_muns[i])\n",
    "            \n",
    "    \n",
    "\n",
    "# 2. Municipios que no tienen nombre de depto\n",
    "for i, item in enumerate(i_muns):\n",
    "    if '-' in item: continue\n",
    "    string = i_muns[i].replace(\"MUNICIPIO DE \", \"\")  # remove the 8 from the string borders\n",
    "    string = string.lstrip()\n",
    "    for contm, mun in enumerate(df_deptos['municipio']):\n",
    "        if mun == string:\n",
    "            i_muns[i] = df_deptos['departamento'][contm] + ' - ' 'ALCALDÍA MUNICIPIO DE ' + string\n",
    "\n",
    "i_muns = [s.rstrip() for s in i_muns]\n",
    "i_muns = [re.sub('MUNICIPIO DEL GUAMO','TOLIMA - ALCALDÍA MUNICIPIO DE EL GUAMO',item) for item in i_muns] \n",
    "\n",
    "# Añadir columna nueva al DF\n",
    "dff_invias = dff_invias.assign(nom_raz_soc_stand = i_muns)\n",
    "i_muns = list(set(i_muns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargar código para ASM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESTANDARIZACION DEL OBJETO A CONTRATAR\n",
    "# Taken from: https://stackoverflow.com/questions/5541745/get-rid-of-stopwords-and-punctuation\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "def remove_stopwords(sentence, language):\n",
    "    return [token for token in nltk.word_tokenize(sentence) if token.lower() not in stopwords.words(language)]\n",
    "\n",
    "# Estandariza el detalle del objeto a contratar\n",
    "# Taken from: https://stackoverflow.com/questions/14153364/reorder-string-using-regular-expressions\n",
    "cachedStopWords = stopwords.words(\"spanish\")\n",
    "cachedStopWords = [x.upper() for x in cachedStopWords]\n",
    "def standarize_obj(obj):\n",
    "    nobj = obj.upper()\n",
    "    nobj = ' '.join([word for word in nobj.split() if word not in cachedStopWords])\n",
    "    nobj = unidecode.unidecode(nobj) # Sin tildes (y sin ñ)\n",
    "    nobj = re.sub(r'[^\\w\\s]','',nobj)     # https://stackoverflow.com/questions/265960/best-way-to-strip-punctuation-from-a-string-in-python\n",
    "    nobj = re.sub(' +', ' ', nobj)   \n",
    "    return nobj\n",
    "\n",
    "###### ACA EMPIEZA LA ITERACION GRANDE ###########\n",
    "### INTENTO CON  TD-IDF\n",
    "        ### https://bergvca.github.io/2017/10/14/super-fast-string-matching.html\n",
    "\n",
    "# 1. Preparar funciones y librerias\n",
    "import re\n",
    "\n",
    "# Define la funcion ngrams que divide el string entre grupos de 3 para \n",
    "# que la comparación sea mucho mas rápida\n",
    "def ngrams(string, n=3):\n",
    "    string = re.sub(r'[,-./]|\\sBD',r'', string)\n",
    "    ngrams = zip(*[string[i:] for i in range(n)])\n",
    "    return [''.join(ngram) for ngram in ngrams]\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import time\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "import sparse_dot_topn.sparse_dot_topn as ct\n",
    "import datetime\n",
    "\n",
    "\n",
    "# En esta funcion se evalua la distancia entre los dos strings con cosine similarity\n",
    "# @ntop: numero de coincidencias que se quieren tener por fila (por string)\n",
    "def awesome_cossim_top(A, B, ntop, lower_bound=0):\n",
    "    # force A and B as a CSR matrix.\n",
    "    # If they have already been CSR, there is no overhead\n",
    "    A = A.tocsr()\n",
    "    B = B.tocsr()\n",
    "    M, _ = A.shape\n",
    "    _, N = B.shape\n",
    " \n",
    "    idx_dtype = np.int32\n",
    " \n",
    "    nnz_max = M*ntop\n",
    " \n",
    "    indptr = np.zeros(M+1, dtype=idx_dtype)\n",
    "    indices = np.zeros(nnz_max, dtype=idx_dtype)\n",
    "    data = np.zeros(nnz_max, dtype=A.dtype)\n",
    "\n",
    "    ct.sparse_dot_topn(\n",
    "        M, N, np.asarray(A.indptr, dtype=idx_dtype),\n",
    "        np.asarray(A.indices, dtype=idx_dtype),\n",
    "        A.data,\n",
    "        np.asarray(B.indptr, dtype=idx_dtype),\n",
    "        np.asarray(B.indices, dtype=idx_dtype),\n",
    "        B.data,\n",
    "        ntop,\n",
    "        lower_bound,\n",
    "        indptr, indices, data)\n",
    "\n",
    "    return csr_matrix((data,indices,indptr),shape=(M,N))\n",
    "\n",
    "# ESTA FUNCION FUE MODIFICADA DE LA FUNCION ORIGINAL DE LA PAG WEB\n",
    "    #En esta función se realiza un DF de la función awesome_cossim_top\n",
    "    # solamente con los valores de imun\n",
    "def get_matches_df(sparse_matrix, name_vector):\n",
    "    non_zeros = sparse_matrix.nonzero()\n",
    "    \n",
    "    sparserows = non_zeros[0]\n",
    "    sparsecols = non_zeros[1]\n",
    "    \n",
    "    nr_matches = sparsecols.size\n",
    "    \n",
    "    left_side = np.empty([nr_matches], dtype=object)\n",
    "    right_side = np.empty([nr_matches], dtype=object)\n",
    "    similairity = np.zeros(nr_matches)\n",
    "    pos_left = np.zeros(nr_matches, dtype=np.int)\n",
    "    pos_right = np.zeros(nr_matches, dtype=np.int)\n",
    "    \n",
    "    for index in range(0, nr_matches):\n",
    "        left_side[index] = name_vector[len(name_vector)-1]\n",
    "        right_side[index] = name_vector[sparsecols[index]]\n",
    "        similairity[index] = sparse_matrix.data[index]\n",
    "        pos_left[index] = sparserows[index]\n",
    "        pos_right[index] = sparsecols[index]\n",
    "    \n",
    "    return pd.DataFrame({'left_side': left_side,\n",
    "                          'right_side': right_side,\n",
    "                           'similairity': similairity,\n",
    "                            'pos_left': pos_left,\n",
    "                            'pos_right': pos_right})\n",
    "\n",
    "\n",
    "def get_matches_df1(sparse_matrix, name_vector):\n",
    "    non_zeros = sparse_matrix.nonzero()\n",
    "    \n",
    "    sparserows = non_zeros[0]\n",
    "    sparsecols = non_zeros[1]\n",
    "    \n",
    "    nr_matches = sparsecols.size\n",
    "    \n",
    "    left_side = np.empty([nr_matches], dtype=object)\n",
    "    right_side = np.empty([nr_matches], dtype=object)\n",
    "    similairity = np.zeros(nr_matches)\n",
    "    pos_left = np.zeros(nr_matches, dtype=np.int)\n",
    "    pos_right = np.zeros(nr_matches, dtype=np.int)\n",
    "    \n",
    "    for index in range(0, nr_matches):\n",
    "        left_side[index] = name_vector[sparserows[index]]\n",
    "        right_side[index] = name_vector[sparsecols[index]]\n",
    "        similairity[index] = sparse_matrix.data[index]\n",
    "        pos_left[index] = sparserows[index]\n",
    "        pos_right[index] = sparsecols[index]\n",
    "    \n",
    "    return pd.DataFrame({'left_side': left_side,\n",
    "                          'right_side': right_side,\n",
    "                           'similairity': similairity,\n",
    "                            'pos_left': pos_left,\n",
    "                            'pos_right': pos_right})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ASM ALGORITHM - ALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2. Ejecutar el código\n",
    "fallos = []\n",
    "cinter = pd.DataFrame()\n",
    "fallidos = 0 \n",
    "cont = 0\n",
    "t_proceso = 10000000\n",
    "threshold = 0.8\n",
    "tol_imun = 0.9\n",
    "contador = 1 \n",
    "cont1 = 0\n",
    "continv = 0\n",
    "ti_dfinvias = []\n",
    "ti_api = []\n",
    "ti_token = []\n",
    "ti_matrix = []\n",
    "ti_printdb = []\n",
    "ti_append = []\n",
    "ti_fuzzy = []\n",
    "ti_t = []\n",
    "sorter = ['Liquidado', 'Terminado Sin Liquidar', 'Celebrado', 'Adjudicado', 'Convocado']\n",
    "\n",
    "start = time.time()\n",
    "for i, item in enumerate(i_muns):\n",
    "    t_b = time.time()\n",
    "    print (\"Iteracion\")\n",
    "    print (i)\n",
    "   # Subsetting INVIAS df\n",
    "    df_imun = dff_invias.loc[dff_invias['nom_raz_soc_stand'] == item]\n",
    "    df_imun = df_imun[df_imun['estado_del_proceso'].isin(sorter)]\n",
    "    df_imun = df_imun.reset_index(drop=True)  \n",
    "    df_imun['duplicated'] = np.zeros(len(df_imun), dtype=np.int)\n",
    "    df_imun['final'] = np.ones(len(df_imun), dtype=bool)\n",
    "    # Crear vector de detalles de objeto a contratar\n",
    "    l_imun = df_imun['detalle_del_objeto_a_contratar']\n",
    "    l_imun = list(map(str, l_imun))\n",
    "    l_imun = pd.Series(l_imun)\n",
    "    # Ejecutar matriz de comparación\n",
    "    vectorizer1 = TfidfVectorizer(min_df=1, analyzer=ngrams)\n",
    "    tf_idf_matrix1 = vectorizer1.fit_transform(l_imun)         #transforma a matriz\n",
    "    matches1 = awesome_cossim_top(tf_idf_matrix1, tf_idf_matrix1.transpose(), 4, 0.0)   #max se repite 4 veces un contrato\n",
    "    matches_df1 = get_matches_df1(matches1, l_imun)\n",
    "    temp = -1\n",
    "    for indexf, fila in matches_df1.iterrows():\n",
    "        if temp < fila['pos_left']:\n",
    "            temp = fila['pos_left']\n",
    "            continv = continv + 1\n",
    "        if fila['pos_left'] == fila['pos_right']: \n",
    "            continue\n",
    "        elif fila['similairity'] > tol_imun:\n",
    "            df_imun['duplicated'].loc[fila['pos_right']] = continv\n",
    "            df_imun['duplicated'].loc[fila['pos_left']] = continv\n",
    "        else:\n",
    "            df_imun['duplicated'].loc[fila['pos_left']] = continv\n",
    "        temp = fila['pos_left']\n",
    "    # https://stackoverflow.com/questions/23482668/sorting-by-a-custom-list-in-pandas\n",
    "    df_imun.estado_del_proceso = df_imun.estado_del_proceso.astype(\"category\")\n",
    "    df_imun.estado_del_proceso.cat.set_categories(sorter, inplace=True)\n",
    "    for contador in range (1,max(df_imun['duplicated'])+1):\n",
    "        if len(df_imun[df_imun['duplicated'] == contador]) > 1:    \n",
    "            tdf = df_imun[df_imun['duplicated'] == contador]\n",
    "            tdf.sort_values([\"estado_del_proceso\"], inplace = True)\n",
    "            c = 0 \n",
    "            for index, row in tdf.iterrows():\n",
    "                if c == 0: \n",
    "                    df_imun['final'].loc[index] = True \n",
    "                    c = c+1\n",
    "                else: df_imun['final'].loc[index] = False\n",
    "        else:\n",
    "            df_imun['final'][df_imun['duplicated'] == contador] = True \n",
    "    \n",
    "    t_invias = time.time()\n",
    "    tinvias= t_invias - t_b\n",
    "    ti_dfinvias.append(tinvias)\n",
    "    \n",
    "    # Getting API from municipio \n",
    "    p_mun = {'nombre_de_la_entidad': i_muns[i], '$limit': 1000000}\n",
    "    r_mun = requests.get(url_secop, params = p_mun)\n",
    "    d_mun = r_mun.json()   # To .json\n",
    "    df_mun = pd.DataFrame(d_mun)    # To df\n",
    "    if df_mun.empty:\n",
    "        fallos.append(i_muns[i])\n",
    "        print (i_muns[i])\n",
    "        fallidos = fallidos + 1\n",
    "        print (\"DB MUN Empty\")\n",
    "        print(\"-----------------------\")\n",
    "        ti_api.append(0)\n",
    "        ti_fuzzy.append(0)\n",
    "        ti_t.append(time.time()-t_b)\n",
    "        continue\n",
    "    df_mun = df_mun[df_mun['estado_del_proceso'].isin(sorter)]\n",
    "    if df_mun.empty: continue\n",
    "    df_mun[['cuantia_proceso']] = df_mun[['cuantia_proceso']].apply(pd.to_numeric)\n",
    "    df_mun[['anno_firma_del_contrato']] = df_mun[['anno_firma_del_contrato']].apply(pd.to_numeric)\n",
    "    df_mun = df_mun.reset_index(drop=True)\n",
    "    print (i_muns[i])\n",
    "    \n",
    "    df_imun[['cuantia_proceso']] = df_imun[['cuantia_proceso']].apply(pd.to_numeric)\n",
    "    df_imun[['anno_firma_del_contrato']] = df_imun[['anno_firma_del_contrato']].apply(pd.to_numeric)\n",
    "    df_imun = df_imun.reset_index(drop=True) \n",
    "    \n",
    "    t_api = time.time()\n",
    "    print('Time API: ')\n",
    "    tapi= t_api - t_invias\n",
    "    ti_api.append(tapi)\n",
    "    print(tapi)\n",
    "    \n",
    "    \n",
    "    # TD-IDF String Matching\n",
    "    for index, row in df_imun.iterrows():    \n",
    "        # No iterar en los que tienen falso\n",
    "        if row['final'] == False: \n",
    "            df1 = row.to_frame().T\n",
    "            df1.index = [cont]\n",
    "            cinter = cinter.append(df1, sort = False)\n",
    "            cont = cont + 1\n",
    "            continue\n",
    "            \n",
    "        c_proceso = row['cuantia_proceso']\n",
    "        if c_proceso == 0: continue\n",
    "        # Subset reglas (temporalidad, valores)\n",
    "        anno_imun = row['anno_firma_del_contrato']\n",
    "        df_mun_f = df_mun[(df_mun.anno_firma_del_contrato >= anno_imun)|(df_mun['anno_firma_del_contrato'].isnull())]\n",
    "        df_mun_f = df_mun_f.reset_index(drop=True) \n",
    "         \n",
    "        \n",
    "        # Crear vector de detalles de objeto a contratar\n",
    "        b = df_mun_f['detalle_del_objeto_a_contratar']\n",
    "        b = list(map(str, b))\n",
    "        b = list(map(standarize_obj, b))\n",
    "        b = pd.Series(b)\n",
    "        a = pd.Series(standarize_obj(row['detalle_del_objeto_a_contratar']))\n",
    "        objeto = list(b) + list(a)\n",
    "        # Ejecutar matriz de comparación\n",
    "        vectorizer = TfidfVectorizer(min_df=1, analyzer=ngrams)\n",
    "        tf_idf_matrixO = vectorizer.fit_transform(objeto)         #transforma a matriz\n",
    "        \n",
    "        t_token = time.time()\n",
    "        ttoken= t_token - t_api\n",
    "        ti_token.append(ttoken)\n",
    "        \n",
    "        matches = awesome_cossim_top(tf_idf_matrixO, tf_idf_matrixO.transpose(), 3, 0)\n",
    "        \n",
    "        t_matrix = time.time()\n",
    "        tmatrix= t_matrix - t_token\n",
    "        ti_matrix.append(tmatrix)\n",
    "        \n",
    "        #tf_idf_matrixA = vectorizer.fit_transform(a)         #transforma a matriz\n",
    "        #tf_idf_matrixB = vectorizer.fit_transform(b)\n",
    "        #matches = awesome_cossim_top(tf_idf_matrixA, tf_idf_matrixB.transpose(), 10, 0)\n",
    "        #Se obtiene el df con la informacion de los puntajes mas altos\n",
    "        matches_df = get_matches_df1(matches, objeto) \n",
    "        matches_df = matches_df.loc[matches_df['left_side'] == a[0]]\n",
    "        matches_df = matches_df.loc[matches_df['pos_left'] == (len(objeto)-1)]\n",
    "        \n",
    "        t_db = time.time()\n",
    "        tdb= t_db - t_matrix\n",
    "        ti_printdb.append(tdb)     \n",
    "        \n",
    "        for index_ci, fila_ci in matches_df.iterrows():\n",
    "            if fila_ci['pos_left'] == fila_ci['pos_right']: continue    #Si es él mismo no cuenta\n",
    "            score = fila_ci['similairity']\n",
    "            df1 = row.to_frame().T\n",
    "            df2 = df_mun_f.loc[fila_ci['pos_right']].to_frame().T\n",
    "            df1.index = [cont]\n",
    "            df2.index = [cont]\n",
    "            df2.columns = [str(col) + '_m' for col in df2.columns]\n",
    "            result = pd.concat([df1, df2], axis=1, join = 'inner', sort = True)       #Concatena la info de SECOP\n",
    "            cinter = cinter.append(result, sort = False)\n",
    "            cinter.at[cinter.index[cont], 'score'] = score\n",
    "            if score > threshold: cinter.at[cinter.index[cont], 'valido'] = True\n",
    "            else: cinter.at[cinter.index[cont], 'valido'] = False\n",
    "            cont = cont + 1\n",
    "        t_app = time.time()\n",
    "        tapp = t_app - t_db\n",
    "        ti_append.append(tapp)\n",
    "                \n",
    "    t_fuzzy = time.time()\n",
    "    print('Time Fuzzy: ')\n",
    "    tfuzzy=t_fuzzy-t_api\n",
    "    ti_fuzzy.append(tfuzzy)\n",
    "    print(tfuzzy)\n",
    "    \n",
    "    print('TOTAL TIME: ')\n",
    "    tt = t_fuzzy - t_b\n",
    "    ti_t.append(tt)\n",
    "    print(tt)\n",
    "    print(\"-----------------------\")\n",
    "\n",
    "end = time.time()\n",
    "total = end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despues del algoritmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cinterbu = cinter\n",
    "\n",
    "df_tiempos1 = pd.DataFrame({'dfinvias':ti_dfinvias,\n",
    "                           'api':ti_api,\n",
    "                          'fuzzy':ti_fuzzy,\n",
    "                          'total':ti_t})\n",
    "\n",
    "df_tiempos2 = pd.DataFrame({'token':ti_token,\n",
    "                          'matrix':ti_matrix,\n",
    "                          'printdb':ti_printdb,\n",
    "                          'append':ti_append})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cinter.to_csv(\"CI_INVIAS_all.csv\", sep='\\t', decimal = ',', encoding='latin1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tiempos1.to_csv(\"t_INVIAS1_all.csv\", sep='\\t', decimal = ',', encoding='latin1')\n",
    "df_tiempos2.to_csv(\"t_INVIAS2_all.csv\", sep='\\t', decimal = ',', encoding='latin1')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
